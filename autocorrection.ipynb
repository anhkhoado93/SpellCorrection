{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from params import DEVICE, BERT_PRETRAINED\n",
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int = 256, \n",
    "                dropout: float = 0.1, \n",
    "                max_len: int = 400):\n",
    "\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(dim=1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(100000) / d_model))\n",
    "        self.position_encoding = torch.zeros(max_len, d_model).to(device)\n",
    "        self.position_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.position_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"x: shape [batch_size, seq_length, embedding_dim] --> return [batch_size, seq_length, embedding_dim]\"\"\"\n",
    "        x += self.position_encoding[:x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions.\n",
    "def generate_square_mask(sequence_size: int):\n",
    "    mask = (torch.triu(torch.ones((sequence_size, sequence_size), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_source_mask(src: Tensor, mask_token_id: int):\n",
    "    src_mask = (src == mask_token_id)\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEncoderTransformers(nn.Module):\n",
    "    def __init__(self, n_chars: int, \n",
    "                mask_token_id: int, \n",
    "                d_model: int = 256, \n",
    "                d_hid: int = 256, \n",
    "                n_head: int = 4,\n",
    "                n_layers: int = 4,\n",
    "                dropout: float = 0.2):\n",
    "        super(CharEncoderTransformers, self).__init__()\n",
    "        self.position_encoding = PositionalEncoding(d_model, dropout, 512)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "        self.char_embedding = nn.Embedding(n_chars, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.max_char = 50\n",
    "        self.linear_char = nn.Linear(self.max_char * self.d_model, self.d_model)\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_range = 0.1\n",
    "        self.char_embedding.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def merge_embedding(self, embeddings: Tensor, sequence_split, mode='linear') -> Tensor:\n",
    "        \"\"\"\n",
    "        :param embeddings: chars embedding [batch_size, length_seq, d_hid]\n",
    "        :param sequence_split: number character for each word list[int]\n",
    "        :param mode: calculate average or add embedding\n",
    "        :return: [batch_size, num_words, embedding_dim]\n",
    "        \"\"\"\n",
    "        original_sequence_split = sequence_split.copy()\n",
    "        sequence_split = [value + 1 for value in sequence_split]  # plus space\n",
    "        sequence_split[-1] -= 1  # remove for the last token\n",
    "        embeddings = embeddings[:sum(sequence_split)]\n",
    "        embeddings = torch.split(embeddings, sequence_split, dim=0)\n",
    "        embeddings = [embedd[:-1, :] if i != (len(sequence_split) - 1) else embedd for i, embedd in\n",
    "                      enumerate(embeddings)]\n",
    "\n",
    "        if mode == 'avg':\n",
    "            embeddings = pad_sequence(embeddings, padding_value=0, batch_first=True)  # n_word*max_length*d_hid\n",
    "            seq_splits = torch.tensor(original_sequence_split).reshape(-1, 1).to(device)\n",
    "            outs = torch.div(torch.sum(embeddings, dim=1), seq_splits)\n",
    "        elif mode == 'add':\n",
    "            embeddings = pad_sequence(embeddings, padding_value=0, batch_first=True)  # n_word*max_length*d_hid\n",
    "            outs = torch.sum(embeddings, dim=1)\n",
    "        elif mode == 'linear':\n",
    "            embeddings =[\n",
    "                torch.cat(\n",
    "                    (\n",
    "                        embedding_tensor.reshape(-1),\n",
    "                        torch.tensor(\n",
    "                            [0] * (self.max_char - embedding_tensor.size(0)) * self.d_model,\n",
    "                            dtype=torch.long\n",
    "                        ).to(device)\n",
    "                    )\n",
    "                )\n",
    "                for embedding_tensor in embeddings\n",
    "            ]\n",
    "            embeddings = torch.stack(embeddings, dim=0)\n",
    "            outs = self.linear_char(embeddings)\n",
    "        else:\n",
    "            raise Exception('Not Implemented')\n",
    "        return outs\n",
    "\n",
    "    def forward(self, src: Tensor,\n",
    "                batch_splits,\n",
    "                src_mask: Tensor = None,\n",
    "                src_key_padding_mask: Tensor = None\n",
    "                ) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param src: char token ids [batch_size, max_len(setence_batch)]\n",
    "        :param batch_splits:\n",
    "        :param src_mask:\n",
    "        :param src_key_padding_mask: mask pad token\n",
    "        :return: word embedding after combine from char embedding [batch_size*n_words*d_hid]\n",
    "        \"\"\"\n",
    "        src_embeddings = self.char_embedding(src)  # batch_size * len_seq * embedding_dim\n",
    "        src_embeddings = self.position_encoding(src_embeddings)\n",
    "        if src_mask is None or src_mask.size(0) != src.size(1):\n",
    "            src_mask = generate_square_mask(src.size(1))\n",
    "\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = generate_source_mask(src, self.mask_token_id)\n",
    "\n",
    "        outputs = self.transformer_encoder(\n",
    "            src_embeddings.transpose(0, 1),\n",
    "            mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        ).transpose(0, 1)  # batch_size*len(sentence)*d_hid\n",
    "        outputs = pad_sequence(\n",
    "            [self.merge_embedding(embedding, sequence_split) for embedding, sequence_split in\n",
    "             zip(outputs, batch_splits)],\n",
    "            padding_value=0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharWordTransformerEncoding(nn.Module):\n",
    "    def __init__(self, n_words: int, \n",
    "                n_chars: int, \n",
    "                n_label_errors: int,\n",
    "                mask_token_id: int,\n",
    "                use_detection_context: bool = True, \n",
    "                d_model: int = 512, \n",
    "                d_hid: int = 768,\n",
    "                n_head: int = 12, \n",
    "                n_layers: int = 12, \n",
    "                dropout: float = 0.2):\n",
    "        super(CharWordTransformerEncoding, self).__init__()\n",
    "        self.position_encoding = PositionalEncoding(d_model, dropout, 256)\n",
    "        self.char_transformer_encoder = CharEncoderTransformers(n_chars, mask_token_id)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model + self.char_transformer_encoder.d_model, n_head, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "        self.word_embedding = nn.Embedding(n_words, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.use_detection_context = use_detection_context\n",
    "        self.fc1 = nn.Linear(d_hid, n_label_errors)\n",
    "        if use_detection_context:\n",
    "            self.softmax = nn.Softmax(dim=-1)\n",
    "            self.linear_detection_context = nn.Linear(n_label_errors, 40)\n",
    "            self.d_out_hid = d_hid + 40\n",
    "        else:\n",
    "            self.d_out_hid = d_hid\n",
    "        self.fc2 = nn.Linear(self.d_out_hid, n_words)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_range = 0.1\n",
    "        self.word_embedding.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, src_word_error_ids: Tensor,\n",
    "                src_char_ids: Tensor,\n",
    "                batch_splits,\n",
    "                src_mask: Tensor = None,\n",
    "                src_key_padding_mask: Tensor = None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param src_word_error_ids: word token ids [batch_size, n_words]\n",
    "        :param src_char_ids: char token ids [batch_size, seq_len]\n",
    "        :param batch_splits:\n",
    "        :param src_mask:\n",
    "        :param src_key_padding_mask: mask pad token\n",
    "        :return: detection outputs [batch_size * n_words * n_errors] and correction outputs [batch_size * n_words * n_words]\n",
    "        \"\"\"\n",
    "        src_word_embeddings = self.word_embedding(src_word_error_ids)\n",
    "        src_word_embeddings = self.position_encoding(src_word_embeddings)\n",
    "        src_words_from_chars = self.char_transformer_encoder(src_char_ids, batch_splits)  # batch_size*n_words*d_model_char\n",
    "        src_word_embeddings = torch.cat((src_word_embeddings, src_words_from_chars), dim=-1)  # batch_size*n_words*(d_model_char+d_model_word)\n",
    "        # if src_mask is None or src_mask.size(0) != src_word_error_ids.size(1):  # sequence_size\n",
    "        #     src_mask = generate_square_mask(src_word_error_ids.size(1))\n",
    "\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = generate_source_mask(src_word_error_ids, self.mask_token_id)\n",
    "\n",
    "        outputs = self.transformer_encoder(\n",
    "            src_word_embeddings.transpose(0, 1),  # n_words * batch_size * hidden_size\n",
    "            # mask=src_mask,  # n_words * n_words\n",
    "            src_key_padding_mask=src_key_padding_mask  # batch_size * n_words * hidden_size\n",
    "        ).transpose(0, 1)  # batch_size * n_words * d_hid\n",
    "        detection_outputs = self.fc1(outputs)  # batch_size * n_words * n_errors\n",
    "        if self.use_detection_context:\n",
    "            detection_context = self.softmax(detection_outputs)\n",
    "            detection_context = self.linear_detection_context(detection_context)  # batch_size * n_words * d_hid\n",
    "            outputs = torch.cat((outputs, detection_context), dim=-1)  # batch_size * n_words * d_out_hid\n",
    "\n",
    "        correction_outputs = self.fc2(outputs)\n",
    "        return detection_outputs, correction_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1df03152e329f513c7cc2eb5eef2b3f13e08646f6eedb88b7b0e0348ab747516"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('hoangtv_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
